\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{float}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{hyperref}

\geometry{top=2cm, bottom=2cm, left=1.5cm, right=1.5cm}
\graphicspath{{../data/}}

\lstset{
    basicstyle=\ttfamily\scriptsize,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    showstringspaces=false,
    captionpos=b,
    language=C++
}

\title{Massively Parallel Symbolic Regression: A CUDA-Accelerated Genetic Programming Architecture}
\author{Student: Pedro Fanica (54346)\\Course: Parallel and Concurrent Programming\\Prof: Wellington Oliveira\\Faculdade de Ciências da Universidade de Lisboa}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report details the implementation of a CUDA-accelerated evaluation engine for Symbolic Regression. Addressing specific architectural challenges, I designed and implemented a ``Mega-Kernel'' approach that fuses thousands of population individuals into a single kernel launch. The report explicitly addresses the assignment requirements regarding parallelization strategies, memory management, and warp divergence. Experimental results on a dataset of $N=10^8$ rows demonstrate that while Naive CuPy offers competitive performance at mid-range data sizes ($10^7$), the Mega-Kernel architecture scales superiorly at the extremes, achieving a speedup of $>1000\times$ over the CPU baseline.
\end{abstract}

\section{Introduction}
Genetic Programming (GP) requires evaluating thousands of candidate functions against large datasets (fitness evaluation). This process is the primary bottleneck, often consuming $>95\%$ of runtime. Sequential CPU execution is insufficient for modern scale. This report evaluates three implementation strategies: Sequential CPU, Naive CuPy (high-level API), and a Custom ``Mega-Kernel'' CUDA approach, specifically answering architectural questions regarding parallelization and memory hierarchy.

\section{Architectural Implementation}

\subsection{Parallelization Strategy}
To maximize GPU throughput, I employed a hybrid parallelism strategy that maps the GP workload to the GPU grid hierarchy:
\begin{enumerate}
    \item \textbf{Task Parallelism (Grid Y-Dimension):} Each block in the grid is assigned a specific \textit{function} (individual) from the population. The function index is derived from \texttt{blockIdx.y}.
    \item \textbf{Data Parallelism (Grid X-Dimension \& Threads):} Threads within a block parallelize the loop over the dataset rows. The data index is calculated as \texttt{int idx = threadIdx.x}.
\end{enumerate}

\subsection{Kernel and Thread Configuration}
I utilized a block size of \textbf{256 threads} (\texttt{blockDim.x = 256}). This is a heuristic choice that provides sufficient warp occupancy (8 warps per block) to hide memory latency without exhausting register files.
\begin{itemize}
    \item \textbf{Grid-Stride Loop:} Since the dataset size ($N$) often exceeds 256, I implemented a Grid-Stride Loop. Threads iterate over the data with a stride of \texttt{blockDim.x}, allowing the kernel to handle arbitrary dataset sizes without reconfiguration.
\end{itemize}

\subsection{Minimizing Kernels and Data Transfers}
\textbf{Minimizing Data Transfers:}
Data transfer between Host (CPU) and Device (GPU) is the slowest operation.
\begin{itemize}
    \item \textbf{Strategy:} I copy the training data (\texttt{data\_gpu} and \texttt{y\_gpu}) to the GPU \textbf{exactly once} at the start. These arrays persist on VRAM. Only the resulting \texttt{fitness\_scores} are transferred back to the CPU.
    \item \textbf{Data Layout:} I utilize a \textbf{Column-Major} layout. This ensures that when threads in a warp read a feature (e.g., \texttt{x[0]}), they access contiguous memory addresses, enabling coalesced memory transactions.
\end{itemize}

\textbf{Minimizing Kernel Calls (Kernel Fusion):}
A naive approach launches one kernel per operator. For a population of 1000 and average size 50, this would require 50,000 launches.
\begin{itemize}
    \item \textbf{Strategy:} I implemented \textbf{Kernel Fusion} via dynamic transpilation. A Python transpiler converts symbolic strings into C++ CUDA code (Listing \ref{lst:transpiler}), injected into a \texttt{switch-case} statement inside a single kernel.
\end{itemize}

\begin{lstlisting}[language=Python, caption=Dynamic Transpiler Logic, label={lst:transpiler}]
def transpile_to_cuda(line, features):
    for i, c in enumerate(features):
        line = line.replace(f"_{c}_", 
            f"data[idx + {i} * n_rows]")
    # Map safe operators
    return line.replace("sqrtf", "p_sqrtf")
\end{lstlisting}

\subsection{Shared Memory and Reduction}
I used \textbf{Shared Memory} to perform the parallel reduction of the Mean Squared Error (MSE). Instead of global memory atomics, each thread calculates a partial sum in a register. These are reduced in a binary tree within shared memory (Listing \ref{lst:shared}).

\begin{lstlisting}[caption=Shared Memory Reduction, label={lst:shared}]
    __shared__ float sdata[256];
    sdata[tid] = error_sum;
    __syncthreads();
    // Parallel reduction in shared memory
    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) { sdata[tid] += sdata[tid + s]; }
        __syncthreads();
    }
\end{lstlisting}

\subsection{Handling Branch Conflicts (Warp Divergence)}
\textbf{Solution:} The design avoids divergence entirely for the function logic. The switch statement depends on \texttt{func\_idx}, which corresponds to \texttt{blockIdx.y}. Since all threads in a block share the same \texttt{blockIdx.y}, every thread in the warp executes the exact same branch, maintaining SIMT efficiency.

\section{Methodology}

\subsection{Implementation Strategies}
\subsubsection{Sequential CPU}
A baseline Python implementation using \texttt{numpy} and \texttt{eval()}. To handle domain errors common in random genetic trees (e.g., $\sqrt{-x}$), I modified the evaluation logic to utilize \texttt{numpy.emath.sqrt}. This function returns complex numbers for negative inputs instead of raising exceptions. Consequently, the fitness function was adapted to calculate the Mean Squared Error using the squared magnitude of the complex residuals (\texttt{np.mean(np.abs(a - b)**2)}). While robust, this method suffers from significant Python interpreter overhead.

\subsubsection{Naive CuPy}
A high-level GPU implementation using \texttt{cupy} arrays. It leverages GPU bandwidth but issues a separate kernel launch for every node in the expression tree (e.g., \texttt{sin}, \texttt{add}). For a population of 1000 individuals with average size 50, this results in 50,000 kernel launches per generation, causing significant driver overhead.

\subsubsection{Mega-Kernel (CUDA)}
My proposed solution uses \texttt{CuPy.RawKernel} to compile the entire population evaluation into a single (or few) kernel launches. The transpiler converts expression strings into C++ CUDA code, injected into a \texttt{switch} statement. This "Kernel Fusion" eliminates launch overhead and keeps data in GPU registers/shared memory.

\section{Experimental Analysis}

\subsection{Setup and Metrics}
All experiments were conducted on the \textbf{Google Colab} platform using a runtime environment equipped with \textbf{Python 3.12.12} and an \textbf{NVIDIA A100-SXM4-40GB} GPU.

I used a dataset varying from $N=10^2$ to $N=10^8$ rows and $D=15$ features. The Sequential CPU baseline was excluded from benchmarks where $N > 10^6$, as the system RAM usage exceeded 10 GB, causing runtime instability. Metrics include Execution Time, Speedup, MSE, Compile Time, and Power Consumption.

\subsection{Performance Benchmarking}
The Mega-Kernel approach consistently outperformed the CPU baseline across all scales.
\begin{itemize}
    \item \textbf{Execution Speed:} For the standard benchmark ($N=100k$), the Mega-Kernel achieved a mean time of \textbf{0.0024s}, compared to \textbf{2.78s} for the CPU.
    \item \textbf{Batch Optimization:} The optimal batch size was found to be \textbf{512 functions} per kernel. This incurred a high initial compilation time ($\approx 108$ seconds) but maximized runtime execution efficiency.
\end{itemize}

\begin{table}[htbp]
    \centering
    \small % Forces uniform small font size
    \caption{Speedup and Accuracy Summary (N=100,000)}
    \label{tab:speedup}
    \input{../data/speedup_summary.tex}
\end{table}

\subsection{Statistical Significance}
Mann-Whitney U tests confirm that the performance differences are statistically significant ($p < 0.05$).

\begin{table}[htbp]
    \centering
    \small % Forces uniform small font size
    \caption{Statistical Significance Tests}
    \label{tab:stats}
    \input{../data/mann_whitney_stats.tex}
\end{table}

\subsection{Scalability Analysis}

\subsubsection{Crossover Point and High-Volume Data}
The GPU outperforms the CPU even at $N=100$. However, a distinct interaction was observed between Naive CuPy and the Mega-Kernel (Figure \ref{fig:crossover}):

\begin{itemize}
    \item \textbf{Low N ($<10^6$):} Mega-Kernel is dominant.
    \item \textbf{The $10^7$ Inversion:} At $N=10^7$, Naive CuPy (0.0289s) briefly outperformed the Mega-Kernel (0.0842s). This is hypothesized to be due to register pressure in the fused kernel reducing occupancy compared to the simpler, dedicated kernels used by Naive CuPy.
    \item \textbf{Massive N ($10^8$):} At the largest scale, the Mega-Kernel (0.82s) regained superiority over Naive CuPy (1.13s), while the CPU implementation failed entirely due to RAM exhaustion.
\end{itemize}

\subsubsection{Feature Scaling}
Figure \ref{fig:features} demonstrates that the Mega-Kernel performance is effectively \textbf{invariant} to feature count. As features increased from 5 to 20, execution time remained stable at $\approx 7 \times 10^{-4}s$.

\subsection{Environmental Impact}
Table \ref{tab:metrics} shows that while the GPU draws more instantaneous power, the massive reduction in execution time results in significantly lower total energy per evaluation.

\begin{table}[htbp]
    \centering
    \small % Forces uniform small font size
    \caption{GPU Environmental Metrics}
    \label{tab:metrics}
    \input{../data/gpu_metrics_table.tex}
\end{table}

\subsection{Compilation Overhead vs. Runtime}
The "tuning" phase revealed a critical trade-off. The high compilation time (108s for batch 512) is mitigated by CuPy's caching mechanism. Subsequent runs utilized the cached kernel, resulting in near-instant load times ($0.0005s$). This validates the architecture for GP, where the same large batch of functions is evaluated repeatedly.

\section{Discussion and Future Work}

\subsection{Optimizing the Genetic Programming Loop}
Integrating this kernel into a full evolutionary loop (Evaluation, Tournament, Crossover, Mutation) requires minimizing Host-Device latency. I propose the following adaptations:
\begin{enumerate}
    \item \textbf{Persistent Data:} Training data remains on VRAM indefinitely, eliminating the $O(N \times D)$ transfer cost per generation.
    \item \textbf{JIT Caching:} The fast, cached compilation must be leveraged. I would employ an ``Elitism'' check to reuse compiled binaries for survivors, avoiding the large initial compilation penalty.
    \item \textbf{Async Execution:} Use \texttt{cp.cuda.Stream} to overlap CPU-heavy tasks (selection logic) with GPU evaluation, hiding the Python interpreter overhead.
\end{enumerate}

\subsection{Future Work: Thread Block Tuning}
In this implementation, the block size was hardcoded to 256 threads. Future work should focus on implementing an \textbf{Occupancy Calculator} to dynamically select the block size (e.g., 128 or 512) that maximizes active warps based on the specific register usage of the generated kernel. This optimization would address the performance dip observed at $N=10^7$.

\section{Conclusion}
This project demonstrated the efficacy of a custom CUDA ``Mega-Kernel'' for accelerating Symbolic Regression. By fusing thousands of candidate functions into single kernel launches, I overcame the latency bottlenecks inherent in high-level libraries and sequential processing. The architecture not only achieved a speedup of over $1000\times$ against the CPU baseline but also enabled the processing of massive datasets ($N=10^8$) that were previously intractable. Although trade-offs regarding compilation time and register pressure were identified at intermediate scales, the proposed solution satisfies all assignment objectives—delivering a robust, scalable, and massively parallel evaluation engine ready for integration into high-performance Genetic Programming workflows.

\newpage
\appendix
\section{Appendix: Figures}
\label{sec:appendix_figures}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{benchmark_plot.png}
    \caption{Performance Distribution (Log Scale)}
    \label{fig:benchmark}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{crossover_plot.png}
    \caption{Crossover Point: Execution Time vs Dataset Size (N)}
    \label{fig:crossover}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{feature_scaling_plot.png}
    \caption{Impact of Feature Count on Performance}
    \label{fig:features}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{tuning_curve.png}
    \caption{Batch Size Tuning Curve: Compile Time vs Execution Time}
    \label{fig:tuning}
\end{figure}

\end{document}